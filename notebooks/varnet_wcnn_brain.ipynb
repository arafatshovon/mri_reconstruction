{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7122286,"sourceType":"datasetVersion","datasetId":4108208},{"sourceId":7487234,"sourceType":"datasetVersion","datasetId":4148070},{"sourceId":7613818,"sourceType":"datasetVersion","datasetId":4294835},{"sourceId":9258165,"sourceType":"datasetVersion","datasetId":5601616},{"sourceId":9503920,"sourceType":"datasetVersion","datasetId":5784325},{"sourceId":9625394,"sourceType":"datasetVersion","datasetId":5866886},{"sourceId":9675064,"sourceType":"datasetVersion","datasetId":5887980},{"sourceId":9802209,"sourceType":"datasetVersion","datasetId":5184815}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2> Necessary Imports </h2>","metadata":{}},{"cell_type":"code","source":"print('''M4Raw, lr = 1e-3, acum = 8, step = in, mod = T1, without DC , for ablation study.''')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:21:25.632309Z","iopub.execute_input":"2025-03-17T19:21:25.632690Z","iopub.status.idle":"2025-03-17T19:21:25.639322Z","shell.execute_reply.started":"2025-03-17T19:21:25.632652Z","shell.execute_reply":"2025-03-17T19:21:25.638461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastmri -q\n!pip install einops -q\n!pip install wandb -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:14:48.680158Z","iopub.execute_input":"2025-03-17T19:14:48.680497Z","iopub.status.idle":"2025-03-17T19:15:17.685334Z","shell.execute_reply.started":"2025-03-17T19:14:48.680465Z","shell.execute_reply":"2025-03-17T19:15:17.684357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom einops import rearrange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.utils.checkpoint as checkpoint\nfrom torch.utils.data import Dataset, DataLoader\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:17.687488Z","iopub.execute_input":"2025-03-17T19:15:17.688255Z","iopub.status.idle":"2025-03-17T19:15:22.055626Z","shell.execute_reply.started":"2025-03-17T19:15:17.688211Z","shell.execute_reply":"2025-03-17T19:15:22.054954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport fastmri\nfrom fastmri.data import transforms\nfrom fastmri import fft2c, ifft2c, rss_complex, complex_abs\nfrom typing import Dict, Optional, Sequence, Tuple, List, Union, NamedTuple, Any, Callable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.056616Z","iopub.execute_input":"2025-03-17T19:15:22.056887Z","iopub.status.idle":"2025-03-17T19:15:22.067656Z","shell.execute_reply.started":"2025-03-17T19:15:22.056862Z","shell.execute_reply":"2025-03-17T19:15:22.066834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport xml.etree.ElementTree as etree\nimport matplotlib.pyplot as plt\nfrom typing import TextIO\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nrandom_seed = 42\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.069385Z","iopub.execute_input":"2025-03-17T19:15:22.069648Z","iopub.status.idle":"2025-03-17T19:15:22.581291Z","shell.execute_reply.started":"2025-03-17T19:15:22.069617Z","shell.execute_reply":"2025-03-17T19:15:22.580476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Data Directory </h2>","metadata":{}},{"cell_type":"markdown","source":"<h4> Single Coil </h4>","metadata":{}},{"cell_type":"code","source":"# acq = ('CORPD_FBK', 'CORPDFS_FBK')\n# df1 = pd.read_csv('/kaggle/input/fastmri-trained-models/train_knee_multi_coil.csv')\n# df2 = pd.read_csv('/kaggle/input/fastmri-trained-models/validation_knee_multi_coil.csv')\n\n# filt1 = df1['acquisition'] == acq[0]\n# filt2 = df2['acquisition'] == acq[0]\n\n# train_file = df1[filt1]['file_name'].values\n# val_file = df2[filt2]['file_name'].values\n# print(len(train_file), len(val_file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.582304Z","iopub.execute_input":"2025-03-17T19:15:22.582813Z","iopub.status.idle":"2025-03-17T19:15:22.611177Z","shell.execute_reply.started":"2025-03-17T19:15:22.582752Z","shell.execute_reply":"2025-03-17T19:15:22.610343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> Multi-coil </h4>","metadata":{}},{"cell_type":"code","source":"# multicoil = []\n# multicoil.extend([os.path.join(path4, f) for f in os.listdir(path4)])\n# multicoil.extend([os.path.join(path5, f) for f in os.listdir(path5)])\n# multicoil.extend([os.path.join(path6, f) for f in os.listdir(path6)])\n# train_file, val_file = train_test_split(multicoil, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.612150Z","iopub.execute_input":"2025-03-17T19:15:22.612375Z","iopub.status.idle":"2025-03-17T19:15:22.616136Z","shell.execute_reply.started":"2025-03-17T19:15:22.612352Z","shell.execute_reply":"2025-03-17T19:15:22.615277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acq = ['AXFLAIR', 'AXT1', 'AXT2']\ndf1 = pd.read_csv('/kaggle/input/fastmri-trained-models/train_M4Raw.csv')\ndf2 = pd.read_csv('/kaggle/input/fastmri-trained-models/val_M4Raw.csv')\nfilt1 = df1['acquisition'] == acq[1]\nfilt2 = df2['acquisition'] == acq[1]\n\ntrain_file = df1[filt1]['file_name'].values\nval_file = df2[filt2]['file_name'].values\nprint(len(train_file), len(val_file))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:22:12.064547Z","iopub.execute_input":"2025-03-17T19:22:12.064932Z","iopub.status.idle":"2025-03-17T19:22:12.082829Z","shell.execute_reply.started":"2025-03-17T19:22:12.064900Z","shell.execute_reply":"2025-03-17T19:22:12.081948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Utils </h2>","metadata":{}},{"cell_type":"code","source":"class Args:\n    def __init__(self, args_dict):\n        for key, value in args_dict.items():\n            setattr(self, key, value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.644098Z","iopub.execute_input":"2025-03-17T19:15:22.644377Z","iopub.status.idle":"2025-03-17T19:15:22.648911Z","shell.execute_reply.started":"2025-03-17T19:15:22.644350Z","shell.execute_reply":"2025-03-17T19:15:22.648038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaskFunc:\n    def __init__(\n        self,\n        center_fractions: Sequence[float],\n        accelerations: Sequence[int],\n        allow_any_combination: bool = False,\n        seed: Optional[int] = None,\n    ):\n        if len(center_fractions) != len(accelerations) and not allow_any_combination:\n            raise ValueError(\n                \"Number of center fractions should match number of accelerations \"\n                \"if allow_any_combination is False.\"\n            )\n\n        self.center_fractions = center_fractions\n        self.accelerations = accelerations\n        self.allow_any_combination = allow_any_combination\n        self.rng = np.random.RandomState(seed)\n\n    def __call__(\n        self,\n        shape: Sequence[int],\n        offset: Optional[int] = None,\n        seed: Optional[Union[int, Tuple[int, ...]]] = None,\n    ) -> Tuple[torch.Tensor, int]:\n        \n        if len(shape) < 3:\n            raise ValueError(\"Shape should have 3 or more dimensions\")\n\n        with temp_seed(self.rng, seed):\n            center_mask, accel_mask, num_low_frequencies = self.sample_mask(\n                shape, offset\n            )\n\n        # combine masks together\n        return torch.max(center_mask, accel_mask), num_low_frequencies\n\n    def sample_mask(\n        self,\n        shape: Sequence[int],\n        offset: Optional[int],\n    ) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \n        num_cols = shape[-2]\n        center_fraction, acceleration = self.choose_acceleration()\n        num_low_frequencies = round(num_cols * center_fraction)\n        center_mask = self.reshape_mask(\n            self.calculate_center_mask(shape, num_low_frequencies), shape\n        )\n        acceleration_mask = self.reshape_mask(\n            self.calculate_acceleration_mask(\n                num_cols, acceleration, offset, num_low_frequencies\n            ),\n            shape,\n        )\n\n        return center_mask, acceleration_mask, num_low_frequencies\n\n    def reshape_mask(self, mask: np.ndarray, shape: Sequence[int]) -> torch.Tensor:\n        \"\"\"Reshape mask to desired output shape.\"\"\"\n        num_cols = shape[-2]\n        mask_shape = [1 for _ in shape]\n        mask_shape[-2] = num_cols\n\n        return torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n\n    def calculate_acceleration_mask(\n        self,\n        num_cols: int,\n        acceleration: int,\n        offset: Optional[int],\n        num_low_frequencies: int,\n    ) -> np.ndarray:\n        \n        raise NotImplementedError\n\n    def calculate_center_mask(\n        self, shape: Sequence[int], num_low_freqs: int\n    ) -> np.ndarray:\n        \n        num_cols = shape[-2]\n        mask = np.zeros(num_cols, dtype=np.float32)\n        pad = (num_cols - num_low_freqs + 1) // 2\n        mask[pad : pad + num_low_freqs] = 1\n        assert mask.sum() == num_low_freqs\n\n        return mask\n\n    def choose_acceleration(self):\n        \"\"\"Choose acceleration based on class parameters.\"\"\"\n        if self.allow_any_combination:\n            return self.rng.choice(self.center_fractions), self.rng.choice(\n                self.accelerations\n            )\n        else:\n            choice = self.rng.randint(len(self.center_fractions))\n            return self.center_fractions[choice], self.accelerations[choice]\n\n\nclass RandomMaskFunc(MaskFunc):\n\n    def calculate_acceleration_mask(\n        self,\n        num_cols: int,\n        acceleration: int,\n        offset: Optional[int],\n        num_low_frequencies: int,\n    ) -> np.ndarray:\n        prob = (num_cols / acceleration - num_low_frequencies) / (\n            num_cols - num_low_frequencies\n        )\n\n        return self.rng.uniform(size=num_cols) < prob\n\n\nclass EquiSpacedMaskFunc(MaskFunc):\n\n    def calculate_acceleration_mask(\n        self,\n        num_cols: int,\n        acceleration: int,\n        offset: Optional[int],\n        num_low_frequencies: int,\n    ) -> np.ndarray:\n        if offset is None:\n            offset = self.rng.randint(0, high=round(acceleration))\n\n        mask = np.zeros(num_cols, dtype=np.float32)\n        mask[offset::acceleration] = 1\n\n        return mask\n    \n\nclass EquispacedMaskFractionFunc(MaskFunc):\n\n    def calculate_acceleration_mask(\n        self,\n        num_cols: int,\n        acceleration: int,\n        offset: Optional[int],\n        num_low_frequencies: int,\n    ) -> np.ndarray:\n        adjusted_accel = (acceleration * (num_low_frequencies - num_cols)) / (\n            num_low_frequencies * acceleration - num_cols\n        )\n        if offset is None:\n            offset = self.rng.randint(0, high=round(adjusted_accel))\n\n        mask = np.zeros(num_cols)\n        accel_samples = np.arange(offset, num_cols - 1, adjusted_accel)\n        accel_samples = np.around(accel_samples).astype(np.uint)\n        mask[accel_samples] = 1.0\n\n        return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.650214Z","iopub.execute_input":"2025-03-17T19:15:22.650434Z","iopub.status.idle":"2025-03-17T19:15:22.667654Z","shell.execute_reply.started":"2025-03-17T19:15:22.650402Z","shell.execute_reply":"2025-03-17T19:15:22.666819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import contextlib\n@contextlib.contextmanager\ndef temp_seed(rng: np.random.RandomState, seed: Optional[Union[int, Tuple[int, ...]]]):\n    \"\"\"A context manager for temporarily adjusting the random seed.\"\"\"\n    if seed is None:\n        try:\n            yield\n        finally:\n            pass\n    else:\n        state = rng.get_state()\n        rng.seed(seed)\n        try:\n            yield\n        finally:\n            rng.set_state(state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.671565Z","iopub.execute_input":"2025-03-17T19:15:22.671826Z","iopub.status.idle":"2025-03-17T19:15:22.683802Z","shell.execute_reply.started":"2025-03-17T19:15:22.671796Z","shell.execute_reply":"2025-03-17T19:15:22.683091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_mask_for_mask_type(\n    mask_type_str: str,\n    center_fractions: Sequence[float],\n    accelerations: Sequence[int],\n) -> MaskFunc:\n    \"\"\"\n    Creates a mask of the specified type.\n\n    Args:\n        center_fractions: What fraction of the center of k-space to include.\n        accelerations: What accelerations to apply.\n\n    Returns:\n        A mask func for the target mask type.\n    \"\"\"\n    if mask_type_str == \"random\":\n        return RandomMaskFunc(center_fractions, accelerations)\n    elif mask_type_str == \"equispaced\":\n        return EquiSpacedMaskFunc(center_fractions, accelerations)\n    elif mask_type_str == \"equispaced_fraction\":\n        return EquispacedMaskFractionFunc(center_fractions, accelerations)\n    elif mask_type_str == \"magic\":\n        return MagicMaskFunc(center_fractions, accelerations)\n    elif mask_type_str == \"magic_fraction\":\n        return MagicMaskFractionFunc(center_fractions, accelerations)\n    else:\n        raise ValueError(f\"{mask_type_str} not supported\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.684977Z","iopub.execute_input":"2025-03-17T19:15:22.685558Z","iopub.status.idle":"2025-03-17T19:15:22.694158Z","shell.execute_reply.started":"2025-03-17T19:15:22.685520Z","shell.execute_reply":"2025-03-17T19:15:22.693386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_tensor(data: np.ndarray) -> torch.Tensor:\n    if np.iscomplexobj(data):\n        data = np.stack((data.real, data.imag), axis=-1)\n\n    return torch.from_numpy(data)\n\ndef tensor_to_complex_np(data: torch.Tensor) -> np.ndarray:\n    return torch.view_as_complex(data).numpy()\n\ndef apply_mask(\n    data: torch.Tensor,\n    mask_func: MaskFunc,\n    offset: Optional[int] = None,\n    seed: Optional[Union[int, Tuple[int, ...]]] = None,\n    padding: Optional[Sequence[int]] = None,\n) -> Tuple[torch.Tensor, torch.Tensor, int]:\n    shape = (1,) * len(data.shape[:-3]) + tuple(data.shape[-3:])\n    mask, num_low_frequencies = mask_func(shape, offset, seed)\n    if padding is not None:\n        mask[..., : padding[0], :] = 0\n        mask[..., padding[1] :, :] = 0  # padding value inclusive on right of zeros\n\n    masked_data = data * mask + 0.0  # the + 0.0 removes the sign of the zeros\n\n    return masked_data, mask, num_low_frequencies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.695140Z","iopub.execute_input":"2025-03-17T19:15:22.695388Z","iopub.status.idle":"2025-03-17T19:15:22.707994Z","shell.execute_reply.started":"2025-03-17T19:15:22.695364Z","shell.execute_reply":"2025-03-17T19:15:22.707087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mask_center(x: torch.Tensor, mask_from: int, mask_to: int) -> torch.Tensor:\n    mask = torch.zeros_like(x)\n    mask[:, :, :, mask_from:mask_to] = x[:, :, :, mask_from:mask_to]\n\n    return mask\n\n\ndef batched_mask_center(\n    x: torch.Tensor, mask_from: torch.Tensor, mask_to: torch.Tensor\n) -> torch.Tensor:\n    if not mask_from.shape == mask_to.shape:\n        raise ValueError(\"mask_from and mask_to must match shapes.\")\n    if not mask_from.ndim == 1:\n        raise ValueError(\"mask_from and mask_to must have 1 dimension.\")\n    if not mask_from.shape[0] == 1:\n        if (not x.shape[0] == mask_from.shape[0]) or (\n            not x.shape[0] == mask_to.shape[0]\n        ):\n            raise ValueError(\"mask_from and mask_to must have batch_size length.\")\n\n    if mask_from.shape[0] == 1:\n        mask = mask_center(x, int(mask_from), int(mask_to))\n    else:\n        mask = torch.zeros_like(x)\n        for i, (start, end) in enumerate(zip(mask_from, mask_to)):\n            mask[i, :, :, start:end] = x[i, :, :, start:end]\n\n    return mask\n\n\ndef center_crop(data: torch.Tensor, shape: Tuple[int, int]) -> torch.Tensor:\n    if not (0 < shape[0] <= data.shape[-2] and 0 < shape[1] <= data.shape[-1]):\n        raise ValueError(\"Invalid shapes.\")\n\n    w_from = (data.shape[-2] - shape[0]) // 2\n    h_from = (data.shape[-1] - shape[1]) // 2\n    w_to = w_from + shape[0]\n    h_to = h_from + shape[1]\n\n    return data[..., w_from:w_to, h_from:h_to]\n\n\ndef complex_center_crop(data: torch.Tensor, shape: Tuple[int, int]) -> torch.Tensor:\n    if not (0 < shape[0] <= data.shape[-3] and 0 < shape[1] <= data.shape[-2]):\n        raise ValueError(\"Invalid shapes.\")\n\n    w_from = (data.shape[-3] - shape[0]) // 2\n    h_from = (data.shape[-2] - shape[1]) // 2\n    w_to = w_from + shape[0]\n    h_to = h_from + shape[1]\n\n    return data[..., w_from:w_to, h_from:h_to, :]\n\n\ndef center_crop_to_smallest(\n    x: torch.Tensor, y: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    smallest_width = min(x.shape[-1], y.shape[-1])\n    smallest_height = min(x.shape[-2], y.shape[-2])\n    x = center_crop(x, (smallest_height, smallest_width))\n    y = center_crop(y, (smallest_height, smallest_width))\n\n    return x, y\n\n\ndef normalize(\n    data: torch.Tensor,\n    mean: Union[float, torch.Tensor],\n    stddev: Union[float, torch.Tensor],\n    eps: Union[float, torch.Tensor] = 0.0,\n) -> torch.Tensor:\n    return (data - mean) / (stddev + eps)\n\n\ndef normalize_instance(\n    data: torch.Tensor, eps: Union[float, torch.Tensor] = 0.0\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    mean = data.mean()\n    std = data.std()\n\n    return normalize(data, mean, std, eps), mean, std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.709281Z","iopub.execute_input":"2025-03-17T19:15:22.709570Z","iopub.status.idle":"2025-03-17T19:15:22.724491Z","shell.execute_reply.started":"2025-03-17T19:15:22.709524Z","shell.execute_reply":"2025-03-17T19:15:22.723719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def et_query(root: etree.Element, qlist: Sequence[str], namespace: str = \"http://www.ismrm.org/ISMRMRD\"):\n    s = \".\"\n    prefix = \"ismrmrd_namespace\"\n    ns = {prefix: namespace}\n    for el in qlist:\n        s = s + f\"//{prefix}:{el}\"\n    value = root.find(s, ns)\n    if value is None:\n        raise RuntimeError(\"Element not found\")\n    return str(value.text)\n\n\nclass FastMRIRawDataSample(NamedTuple):\n    fname: str\n    slice_ind: int\n    metadata: Dict[str, Any]\n\ndef resize_image(image_tensor):\n    image_tensor = transforms.ToPILImage()(image_tensor).resize((img_width, img_height), resample=Image.BICUBIC)\n    image_tensor = transforms.ToTensor()(image_tensor)\n    return image_tensor\n\ndef Raw_sample_filter(raw_sample):\n    return True\n\nclass VarNetSample(NamedTuple):\n    \"\"\"\n    A sample of masked k-space for variational network reconstruction.\n\n    Args:\n        masked_kspace: k-space after applying sampling mask.\n        mask: The applied sampling mask.\n        num_low_frequencies: The number of samples for the densely-sampled\n            center.\n        target: The target image (if applicable).\n        fname: File name.\n        slice_num: The slice index.\n        max_value: Maximum image value.\n        crop_size: The size to crop the final image.\n    \"\"\"\n\n    masked_kspace: torch.Tensor\n    mask: torch.Tensor\n#     num_low_frequencies: Optional[int]\n    target: torch.Tensor\n    fname: str\n    slice_num: int\n    max_value: float\n    crop_size: Tuple[int, int]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.725460Z","iopub.execute_input":"2025-03-17T19:15:22.725721Z","iopub.status.idle":"2025-03-17T19:15:22.737956Z","shell.execute_reply.started":"2025-03-17T19:15:22.725693Z","shell.execute_reply":"2025-03-17T19:15:22.737247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Data Preprocessing </h2>","metadata":{}},{"cell_type":"code","source":"class DataTransform:\n    \"\"\"\n    Data Transformer for training HUMUS-Net model.\n    \"\"\"\n\n    def __init__(self,\n                 uniform_train_resolution: Union[List[int], Tuple[int]],\n                 mask_func: Optional[MaskFunc] = None,\n                 use_seed: bool = True,\n):\n    \n        self.mask_func = mask_func\n        self.use_seed = use_seed\n        self.uniform_train_resolution = uniform_train_resolution\n\n    def _crop_if_needed(self, image):\n        w_from = h_from = 0\n\n        if self.uniform_train_resolution[0] < image.shape[-3]:\n            w_from = (image.shape[-3] - self.uniform_train_resolution[0]) // 2\n            w_to = w_from + self.uniform_train_resolution[0]\n        else:\n            w_to = image.shape[-3]\n\n        if self.uniform_train_resolution[1] < image.shape[-2]:\n            h_from = (image.shape[-2] - self.uniform_train_resolution[1]) // 2\n            h_to = h_from + self.uniform_train_resolution[1]\n        else:\n            h_to = image.shape[-2]\n\n        return image[..., w_from:w_to, h_from:h_to, :]\n\n    def _pad_if_needed(self, image):\n        pad_w = self.uniform_train_resolution[0] - image.shape[-3]\n        pad_h = self.uniform_train_resolution[1] - image.shape[-2]\n\n        if pad_w > 0:\n            pad_w_left = pad_w // 2\n            pad_w_right = pad_w - pad_w_left\n        else:\n            pad_w_left = pad_w_right = 0\n\n        if pad_h > 0:\n            pad_h_left = pad_h // 2\n            pad_h_right = pad_h - pad_h_left\n        else:\n            pad_h_left = pad_h_right = 0\n\n        return torch.nn.functional.pad(image.permute(0, 3, 1, 2), (pad_h_left, pad_h_right, pad_w_left, pad_w_right), 'reflect').permute(0, 2, 3, 1)\n\n    def _to_uniform_size(self, kspace):\n        image = ifft2c(kspace)\n        image = self._crop_if_needed(image)\n        image = self._pad_if_needed(image)\n        kspace = fft2c(image)\n        return kspace\n\n    def __call__(\n        self,\n        kspace: np.ndarray,\n        mask: np.ndarray,\n        target: np.ndarray,\n        attrs: Dict,\n        fname: str,\n        slice_num: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str, int, float, torch.Tensor]:\n        \"\"\"\n        Args:\n            kspace: Input k-space of shape (num_coils, rows, cols) for\n                multi-coil data.\n            mask: Mask from the test dataset.\n            target: Target image.\n            attrs: Acquisition related information stored in the HDF5 object.\n            fname: File name.\n            slice_num: Serial number of the slice.\n        Returns:\n            tuple containing:\n                masked_kspace: k-space after applying sampling mask.\n                mask: The applied sampling mask\n                target: The target image (if applicable).\n                fname: File name.\n                slice_num: The slice index.\n                max_value: Maximum image value.\n                crop_size: The size to crop the final image.\n        \"\"\"\n        is_testing = (target is None)\n\n        kspace = kspace.astype(np.complex64)\n\n        # Add singleton channel dimension if singlecoil\n        if len(kspace.shape) == 2:\n            kspace = np.expand_dims(kspace, axis=0)\n        # Concatenate slices along channel dim if multi-slice\n        elif len(kspace.shape) == 4:\n            H, W = kspace.shape[-2:]\n            kspace = np.reshape(kspace, (-1, H, W))\n        assert len(kspace.shape) == 3\n\n        if not is_testing:\n            target = target.astype(np.float32)\n            target = to_tensor(target)\n#             target, *_ = transforms.normalize_instance(target, eps=1e-11)\n#             target = target.clamp(-1, 1)\n            max_value = attrs[\"max\"].astype(np.float32)\n        else:\n            target = torch.tensor(0)\n            max_value = 0.0\n\n        kspace = to_tensor(kspace)\n\n        if not is_testing:\n            kspace = self._to_uniform_size(kspace)\n        else:\n            # Only crop image height\n            if self.uniform_train_resolution[0] < kspace.shape[-3]:\n                image = ifft2c(kspace)\n                h_from = (image.shape[-3] - self.uniform_train_resolution[0]) // 2\n                h_to = h_from + self.uniform_train_resolution[0]\n                image = image[..., h_from:h_to, :, :]\n                kspace = fft2c(image)\n\n        seed = None if not self.use_seed else tuple(map(ord, fname))\n        acq_start = attrs[\"padding_left\"]\n        acq_end = attrs[\"padding_right\"]\n\n        if not is_testing:\n            crop_size = torch.tensor([target.shape[0], target.shape[1]])\n        else:\n            crop_size = (attrs[\"recon_size\"][0], attrs[\"recon_size\"][1])\n\n        if self.mask_func:\n            masked_kspace, mask, _ = apply_mask(\n                kspace, self.mask_func, seed = seed, padding=(acq_start, acq_end)\n            )\n        else:\n            masked_kspace = kspace\n            shape = np.array(kspace.shape)\n            num_cols = shape[-2]\n            shape[:-3] = 1\n            mask_shape = [1] * len(shape)\n            mask_shape[-2] = num_cols\n            mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n            mask = mask.reshape(*mask_shape)\n            mask[:, :, :acq_start] = 0\n            mask[:, :, acq_end:] = 0\n\n        return (\n            masked_kspace,\n            mask.byte(),\n            target,\n            fname,\n            slice_num,\n            max_value,\n            crop_size,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.738988Z","iopub.execute_input":"2025-03-17T19:15:22.739297Z","iopub.status.idle":"2025-03-17T19:15:22.757296Z","shell.execute_reply.started":"2025-03-17T19:15:22.739261Z","shell.execute_reply":"2025-03-17T19:15:22.756462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Data Loader </h2>","metadata":{}},{"cell_type":"code","source":"class Mri_Data(Dataset):\n    def __init__(self, Files, transform, challenge):\n        self.file = Files\n        self.transform = transform\n        self.challenge = challenge\n        if self.challenge == 'multicoil':\n            self.start = 0\n            self.end = None\n        else:\n            self.start = 10\n            self.end = None\n\n    def _retrive_metadata(self, hf: TextIO):\n        et_root = etree.fromstring(hf[\"ismrmrd_header\"][()])\n\n        enc = [\"encoding\", \"encodedSpace\", \"matrixSize\"]\n        enc_size = (\n            int(et_query(et_root, enc + [\"x\"])),\n            int(et_query(et_root, enc + [\"y\"])),\n            int(et_query(et_root, enc + [\"z\"])),\n        )\n        rec = [\"encoding\", \"reconSpace\", \"matrixSize\"]\n        recon_size = (\n            int(et_query(et_root, rec + [\"x\"])),\n            int(et_query(et_root, rec + [\"y\"])),\n            int(et_query(et_root, rec + [\"z\"])),\n        )\n\n        lims = [\"encoding\", \"encodingLimits\", \"kspace_encoding_step_1\"]\n        enc_limits_center = int(et_query(et_root, lims + [\"center\"]))\n        enc_limits_max = int(et_query(et_root, lims + [\"maximum\"])) + 1\n\n        padding_left = enc_size[1] // 2 - enc_limits_center\n        padding_right = padding_left + enc_limits_max\n\n        num_slices = hf[\"kspace\"].shape[0]\n\n        metadata = {\n            \"padding_left\": padding_left,\n            \"padding_right\": padding_right,\n            \"encoding_size\": enc_size,\n            \"recon_size\": recon_size,\n        }\n        return metadata\n\n\n    def __len__(self):\n        return len(self.file)\n\n\n    def __getitem__(self, index):\n        Masked_kspace, Mask, Target, Slice_num, Max_value, Crop_size = [], [], [], [], [], []\n        with h5py.File(self.file[index], 'r') as hf:\n\n            kspace = hf['kspace']\n            if self.challenge == 'multicoil':\n                rec_esc = hf['reconstruction_rss']\n            else:\n                rec_esc = hf['reconstruction_esc']\n\n            attr = dict(hf.attrs)\n            attr.update(self._retrive_metadata(hf))\n            f_name = self.file[index].split('/')[-1]\n\n            for idx, element in enumerate(kspace):\n#                 if self.challenge == 'multicoil' and element.shape[0] > 16:\n#                     kspace, mask, target, fname, slc_num, val, crop = self.transform(element[:16], None, rec_esc[idx], attr, f_name, idx)\n#                 else:\n                kspace, mask, target, fname, slc_num, val, crop = self.transform(element, None, rec_esc[idx], attr, f_name, idx)\n                Masked_kspace.append(kspace)\n                Mask.append(mask)\n                Target.append(target)\n                Slice_num.append(slc_num)\n                Max_value.append(val)\n                Crop_size.append(crop)\n\n        return (\n            torch.stack(Masked_kspace[self.start:self.end]),\n            torch.stack(Mask[self.start:self.end]),\n            torch.stack(Target[self.start:self.end]),\n            torch.tensor(Slice_num[self.start:self.end]),\n            torch.tensor(Max_value[self.start:self.end]),\n            torch.stack(Crop_size[self.start:self.end])\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.758611Z","iopub.execute_input":"2025-03-17T19:15:22.758962Z","iopub.status.idle":"2025-03-17T19:15:22.777985Z","shell.execute_reply.started":"2025-03-17T19:15:22.758925Z","shell.execute_reply":"2025-03-17T19:15:22.777179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Model(VarNet with WCNN) </h2>","metadata":{}},{"cell_type":"markdown","source":"<h4> WCNN </h4>","metadata":{}},{"cell_type":"code","source":"def default_conv(in_channels, out_channels, kernel_size, bias=True, dilation=1):\n    return nn.Conv2d(\n        in_channels, out_channels, kernel_size,\n        padding=(kernel_size//2)+dilation-1, bias=bias, dilation=dilation)\n\ndef default_conv1(in_channels, out_channels, kernel_size, bias=True, groups=3):\n    return nn.Conv2d(\n        in_channels,out_channels, kernel_size,\n        padding=(kernel_size//2), bias=bias, groups=groups)\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.size()\n\n    channels_per_group = num_channels // groups\n    x = x.view(batchsize, groups,\n               channels_per_group, height, width)\n\n    x = torch.transpose(x, 1, 2).contiguous()\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\ndef pixel_down_shuffle(x, downsacale_factor):\n    batchsize, num_channels, height, width = x.size()\n\n    out_height = height // downsacale_factor\n    out_width = width // downsacale_factor\n    input_view = x.contiguous().view(batchsize, num_channels, out_height, downsacale_factor, out_width,\n                                     downsacale_factor)\n\n    num_channels *= downsacale_factor ** 2\n    unshuffle_out = input_view.permute(0,1,3,5,2,4).contiguous()\n\n    return unshuffle_out.view(batchsize, num_channels, out_height, out_width)\n\n\n\ndef sp_init(x):\n\n    x01 = x[:, :, 0::2, :]\n    x02 = x[:, :, 1::2, :]\n    x_LL = x01[:, :, :, 0::2]\n    x_HL = x02[:, :, :, 0::2]\n    x_LH = x01[:, :, :, 1::2]\n    x_HH = x02[:, :, :, 1::2]\n\n\n    return torch.cat((x_LL, x_HL, x_LH, x_HH), 1)\n\ndef dwt_init(x):\n\n    x01 = x[:, :, 0::2, :] / 2\n    x02 = x[:, :, 1::2, :] / 2\n    x1 = x01[:, :, :, 0::2]\n    x2 = x02[:, :, :, 0::2]\n    x3 = x01[:, :, :, 1::2]\n    x4 = x02[:, :, :, 1::2]\n    x_LL = x1 + x2 + x3 + x4\n    x_HL = -x1 - x2 + x3 + x4\n    x_LH = -x1 + x2 - x3 + x4\n    x_HH = x1 - x2 - x3 + x4\n\n    return torch.cat((x_LL, x_HL, x_LH, x_HH), 1)\n\ndef iwt_init(x,args):\n\n    r = 2\n    in_batch, in_channel, in_height, in_width = x.size()\n    #print([in_batch, in_channel, in_height, in_width])\n    out_batch, out_channel, out_height, out_width = in_batch, int(\n        in_channel / (r ** 2)), r * in_height, r * in_width\n    x1 = x[:, 0:out_channel, :, :] / 2\n    x2 = x[:, out_channel:out_channel * 2, :, :] / 2\n    x3 = x[:, out_channel * 2:out_channel * 3, :, :] / 2\n    x4 = x[:, out_channel * 3:out_channel * 4, :, :] / 2\n\n\n    h = torch.zeros([out_batch, out_channel, out_height, out_width]).float().to(args.device)\n    #print (\"IWT_init:\",args.device)\n\n    h[:, :, 0::2, 0::2] = x1 - x2 - x3 + x4\n    h[:, :, 1::2, 0::2] = x1 - x2 + x3 - x4\n    h[:, :, 0::2, 1::2] = x1 + x2 - x3 - x4\n    h[:, :, 1::2, 1::2] = x1 + x2 + x3 + x4\n\n    return h\n\nclass Channel_Shuffle(nn.Module):\n    def __init__(self, conv_groups):\n        super(Channel_Shuffle, self).__init__()\n        self.conv_groups = conv_groups\n        self.requires_grad = False\n\n    def forward(self, x):\n        return channel_shuffle(x, self.conv_groups)\n\nclass SP(nn.Module):\n    def __init__(self):\n        super(SP, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return sp_init(x)\n\nclass Pixel_Down_Shuffle(nn.Module):\n    def __init__(self):\n        super(Pixel_Down_Shuffle, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return pixel_down_shuffle(x, 2)\n\nclass DWT(nn.Module):\n    def __init__(self):\n        super(DWT, self).__init__()\n        self.requires_grad = False\n\n    def forward(self, x):\n        return dwt_init(x)\n\nclass IWT(nn.Module):\n    def __init__(self,args):\n        super(IWT, self).__init__()\n        self.args = args\n        self.requires_grad = False\n\n    def forward(self, x):\n        return iwt_init(x,self.args)\n\n\nclass MeanShift(nn.Conv2d):\n    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n        std = torch.Tensor(rgb_std)\n        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n        self.weight.data.div_(std.view(3, 1, 1, 1))\n        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n        self.bias.data.div_(std)\n        self.requires_grad = False\n        if sign==-1:\n            self.create_graph = False\n            self.volatile = True\nclass MeanShift2(nn.Conv2d):\n    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n        super(MeanShift2, self).__init__(4, 4, kernel_size=1)\n        std = torch.Tensor(rgb_std)\n        self.weight.data = torch.eye(4).view(4, 4, 1, 1)\n        self.weight.data.div_(std.view(4, 1, 1, 1))\n        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n        self.bias.data.div_(std)\n        self.requires_grad = False\n        if sign==-1:\n            self.volatile = True\n\nclass BasicBlock(nn.Sequential):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, bias=False,\n        bn=False, act=nn.ReLU(True)):\n\n        m = [nn.Conv2d(\n            in_channels, out_channels, kernel_size,\n            padding=(kernel_size//2), stride=stride, bias=bias)\n        ]\n        if bn: m.append(nn.BatchNorm2d(out_channels))\n        if act is not None: m.append(act)\n        super(BasicBlock, self).__init__(*m)\n\nclass BBlock(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(BBlock, self).__init__()\n        m = []\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x).mul(self.res_scale)\n        return x\n\nclass DBlock_com(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_com, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=3))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass DBlock_inv(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_inv, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=3))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass DBlock_com1(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_com1, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=1))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass DBlock_inv1(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_inv1, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=1))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass DBlock_com2(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_com2, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass DBlock_inv2(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DBlock_inv2, self).__init__()\n        m = []\n\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias, dilation=2))\n        if bn: m.append(nn.BatchNorm2d(out_channels, eps=1e-4, momentum=0.95))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x)\n        return x\n\nclass ShuffleBlock(nn.Module):\n    def __init__(\n        self, conv, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1,conv_groups=1):\n\n        super(ShuffleBlock, self).__init__()\n        m = []\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias))\n        m.append(Channel_Shuffle(conv_groups))\n        if bn: m.append(nn.BatchNorm2d(out_channels))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x).mul(self.res_scale)\n        return x\n\n\nclass DWBlock(nn.Module):\n    def __init__(\n        self, conv, conv1, in_channels, out_channels, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(DWBlock, self).__init__()\n        m = []\n        m.append(conv(in_channels, out_channels, kernel_size, bias=bias))\n        if bn: m.append(nn.BatchNorm2d(out_channels))\n        m.append(act)\n\n        m.append(conv1(in_channels, out_channels, 1, bias=bias))\n        if bn: m.append(nn.BatchNorm2d(out_channels))\n        m.append(act)\n\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        x = self.body(x).mul(self.res_scale)\n        return x\n\nclass ResBlock(nn.Module):\n    def __init__(\n        self, conv, n_feat, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(ResBlock, self).__init__()\n        m = []\n        for i in range(2):\n            m.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n            if bn: m.append(nn.BatchNorm2d(n_feat))\n            if i == 0: m.append(act)\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.body(x).mul(self.res_scale)\n        res += x\n\n        return res\n\nclass Block(nn.Module):\n    def __init__(\n        self, conv, n_feat, kernel_size,\n        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(Block, self).__init__()\n        m = []\n        for i in range(4):\n            m.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n            if bn: m.append(nn.BatchNorm2d(n_feat))\n            if i == 0: m.append(act)\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.body(x).mul(self.res_scale)\n        # res += x\n\n        return res\n\nclass Upsampler(nn.Sequential):\n    def __init__(self, conv, scale, n_feat, bn=False, act=False, bias=True):\n\n        m = []\n        if (scale & (scale - 1)) == 0:    # Is scale = 2^n?\n            for _ in range(int(math.log(scale, 2))):\n                m.append(conv(n_feat, 4 * n_feat, 3, bias))\n                m.append(nn.PixelShuffle(2))\n                if bn: m.append(nn.BatchNorm2d(n_feat))\n                if act: m.append(act())\n        elif scale == 3:\n            m.append(conv(n_feat, 9 * n_feat, 3, bias))\n            m.append(nn.PixelShuffle(3))\n            if bn: m.append(nn.BatchNorm2d(n_feat))\n            if act: m.append(act())\n        else:\n            raise NotImplementedError\n\n        super(Upsampler, self).__init__(*m)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.779152Z","iopub.execute_input":"2025-03-17T19:15:22.779403Z","iopub.status.idle":"2025-03-17T19:15:22.829340Z","shell.execute_reply.started":"2025-03-17T19:15:22.779379Z","shell.execute_reply":"2025-03-17T19:15:22.828486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MWCNN(nn.Module):\n\n    def __init__(self, args, conv=default_conv):\n        super(MWCNN, self).__init__()\n        n_feats = 32\n        kernel_size = 3\n        self.scale_idx = 0\n        nColor = 2\n        act = nn.ReLU(True)\n        self.DWT = DWT()\n        self.IWT = IWT(args)\n        n = 1\n        m_head = [BBlock(conv, nColor, n_feats, kernel_size, act=act)]\n        d_l0 = []\n        d_l0.append(DBlock_com1(conv, n_feats, n_feats, kernel_size, act=act, bn=False))\n\n        d_l1 = [BBlock(conv, n_feats * 4, n_feats * 2, kernel_size, act=act, bn=False)]\n        d_l1.append(DBlock_com1(conv, n_feats * 2, n_feats * 2, kernel_size, act=act, bn=False))\n\n        d_l2 = []\n        d_l2.append(BBlock(conv, n_feats * 8, n_feats * 4, kernel_size, act=act, bn=False))\n        d_l2.append(DBlock_com1(conv, n_feats * 4, n_feats * 4, kernel_size, act=act, bn=False))\n        pro_l3 = []\n        pro_l3.append(BBlock(conv, n_feats * 16, n_feats * 8, kernel_size, act=act, bn=False))\n#         uncomment first one\n        pro_l3.append(DBlock_com(conv, n_feats * 8, n_feats * 8, kernel_size, act=act, bn=False))\n#         pro_l3.append(DBlock_inv(conv, n_feats * 8, n_feats * 8, kernel_size, act=act, bn=False))\n        pro_l3.append(BBlock(conv, n_feats * 8, n_feats * 16, kernel_size, act=act, bn=False))\n\n        i_l2 = [DBlock_inv1(conv, n_feats * 4, n_feats * 4, kernel_size, act=act, bn=False)]\n        i_l2.append(BBlock(conv, n_feats * 4, n_feats * 8, kernel_size, act=act, bn=False))\n\n        i_l1 = [DBlock_inv1(conv, n_feats * 2, n_feats * 2, kernel_size, act=act, bn=False)]\n        i_l1.append(BBlock(conv, n_feats * 2, n_feats * 4, kernel_size, act=act, bn=False))\n\n        i_l0 = [DBlock_inv1(conv, n_feats, n_feats, kernel_size, act=act, bn=False)]\n\n        m_tail = [conv(n_feats, nColor, kernel_size)]\n\n        self.head = nn.Sequential(*m_head)\n        self.d_l2 = nn.Sequential(*d_l2)\n        self.d_l1 = nn.Sequential(*d_l1)\n        self.d_l0 = nn.Sequential(*d_l0)\n        self.pro_l3 = nn.Sequential(*pro_l3)\n        self.i_l2 = nn.Sequential(*i_l2)\n        self.i_l1 = nn.Sequential(*i_l1)\n        self.i_l0 = nn.Sequential(*i_l0)\n        self.tail = nn.Sequential(*m_tail)\n\n    def forward(self, x):\n        x0 = self.d_l0(self.head(x))\n        x1 = self.d_l1(self.DWT(x0))\n        x2 = self.d_l2(self.DWT(x1))\n        #print (\"forward device:\",x2.device,dir(self.DWT))\n        x_ = self.IWT(self.pro_l3(self.DWT(x2))) + x2\n        x_ = self.IWT(self.i_l2(x_)) + x1\n        x_ = self.IWT(self.i_l1(x_)) + x0\n        x = self.tail(self.i_l0(x_))# + x #here commented +x since it is taken care in the calling forward method\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.830601Z","iopub.execute_input":"2025-03-17T19:15:22.830918Z","iopub.status.idle":"2025-03-17T19:15:22.843924Z","shell.execute_reply.started":"2025-03-17T19:15:22.830872Z","shell.execute_reply":"2025-03-17T19:15:22.843103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> VarNet </h4>","metadata":{}},{"cell_type":"code","source":"class Unet(nn.Module):\n    \"\"\"\n    PyTorch implementation of a U-Net model.\n\n    O. Ronneberger, P. Fischer, and Thomas Brox. U-net: Convolutional networks\n    for biomedical image segmentation. In International Conference on Medical\n    image computing and computer-assisted intervention, pages 234â€“241.\n    Springer, 2015.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_chans: int,\n        out_chans: int,\n        chans: int = 32,\n        num_pool_layers: int = 4,\n        drop_prob: float = 0.0,\n    ):\n        \"\"\"\n        Args:\n            in_chans: Number of channels in the input to the U-Net model.\n            out_chans: Number of channels in the output to the U-Net model.\n            chans: Number of output channels of the first convolution layer.\n            num_pool_layers: Number of down-sampling and up-sampling layers.\n            drop_prob: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.in_chans = in_chans\n        self.out_chans = out_chans\n        self.chans = chans\n        self.num_pool_layers = num_pool_layers\n        self.drop_prob = drop_prob\n\n        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n        ch = chans\n        for _ in range(num_pool_layers - 1):\n            self.down_sample_layers.append(ConvBlock(ch, ch * 2, drop_prob))\n            ch *= 2\n        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n\n        self.up_conv = nn.ModuleList()\n        self.up_transpose_conv = nn.ModuleList()\n        for _ in range(num_pool_layers - 1):\n            self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n            self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n            ch //= 2\n\n        self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n        self.up_conv.append(\n            nn.Sequential(\n                ConvBlock(ch * 2, ch, drop_prob),\n                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n            )\n        )\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n\n        Returns:\n            Output tensor of shape `(N, out_chans, H, W)`.\n        \"\"\"\n        stack = []\n        output = image\n\n        # apply down-sampling layers\n        for layer in self.down_sample_layers:\n            output = layer(output)\n            stack.append(output)\n            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n\n        output = self.conv(output)\n\n        # apply up-sampling layers\n        for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n            downsample_layer = stack.pop()\n            output = transpose_conv(output)\n\n            # reflect pad on the right/botton if needed to handle odd input dimensions\n            padding = [0, 0, 0, 0]\n            if output.shape[-1] != downsample_layer.shape[-1]:\n                padding[1] = 1  # padding right\n            if output.shape[-2] != downsample_layer.shape[-2]:\n                padding[3] = 1  # padding bottom\n            if torch.sum(torch.tensor(padding)) != 0:\n                output = F.pad(output, padding, \"reflect\")\n\n            output = torch.cat([output, downsample_layer], dim=1)\n            output = conv(output)\n\n        return output\n\n\nclass ConvBlock(nn.Module):\n    \"\"\"\n    A Convolutional Block that consists of two convolution layers each followed by\n    instance normalization, LeakyReLU activation and dropout.\n    \"\"\"\n\n    def __init__(self, in_chans: int, out_chans: int, drop_prob: float):\n        \"\"\"\n        Args:\n            in_chans: Number of channels in the input.\n            out_chans: Number of channels in the output.\n            drop_prob: Dropout probability.\n        \"\"\"\n        super().__init__()\n\n        self.in_chans = in_chans\n        self.out_chans = out_chans\n        self.drop_prob = drop_prob\n\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_chans),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout2d(drop_prob),\n            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_chans),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout2d(drop_prob),\n        )\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n\n        Returns:\n            Output tensor of shape `(N, out_chans, H, W)`.\n        \"\"\"\n        return self.layers(image)\n\n\nclass TransposeConvBlock(nn.Module):\n    \"\"\"\n    A Transpose Convolutional Block that consists of one convolution transpose\n    layers followed by instance normalization and LeakyReLU activation.\n    \"\"\"\n\n    def __init__(self, in_chans: int, out_chans: int):\n        \"\"\"\n        Args:\n            in_chans: Number of channels in the input.\n            out_chans: Number of channels in the output.\n        \"\"\"\n        super().__init__()\n\n        self.in_chans = in_chans\n        self.out_chans = out_chans\n\n        self.layers = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_chans, out_chans, kernel_size=2, stride=2, bias=False\n            ),\n            nn.InstanceNorm2d(out_chans),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n\n        Returns:\n            Output tensor of shape `(N, out_chans, H*2, W*2)`.\n        \"\"\"\n        return self.layers(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.844851Z","iopub.execute_input":"2025-03-17T19:15:22.845098Z","iopub.status.idle":"2025-03-17T19:15:22.861358Z","shell.execute_reply.started":"2025-03-17T19:15:22.845073Z","shell.execute_reply":"2025-03-17T19:15:22.860550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NormUnet(nn.Module):\n    \"\"\"\n    Normalized U-Net model.\n\n    This is the same as a regular U-Net, but with normalization applied to the\n    input before the U-Net. This keeps the values more numerically stable\n    during training.\n    \"\"\"\n\n    def __init__(\n        self,\n        chans: int,\n        num_pools: int,\n        in_chans: int = 2,\n        out_chans: int = 2,\n        drop_prob: float = 0.0,\n    ):\n        \"\"\"\n        Args:\n            chans: Number of output channels of the first convolution layer.\n            num_pools: Number of down-sampling and up-sampling layers.\n            in_chans: Number of channels in the input to the U-Net model.\n            out_chans: Number of channels in the output to the U-Net model.\n            drop_prob: Dropout probability.\n        \"\"\"\n        super().__init__()\n        # self.unet = Unet(\n        #     in_chans=in_chans,\n        #     out_chans=out_chans,\n        #     chans=chans,\n        #     num_pool_layers=num_pools,\n        #     drop_prob=drop_prob,\n        # )\n        \n        \n        self.dwcnn_args = Args({\n        'seed': 42,\n        'num_pools': num_pools,\n        'drop_prob': drop_prob,\n        'num_chans': chans,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu'})\n\n        self.unet = MWCNN(\n            self.dwcnn_args\n        )\n\n    def complex_to_chan_dim(self, x: torch.Tensor) -> torch.Tensor:\n        b, c, h, w, two = x.shape\n        assert two == 2\n        return x.permute(0, 4, 1, 2, 3).reshape(b, 2 * c, h, w)\n\n    def chan_complex_to_last_dim(self, x: torch.Tensor) -> torch.Tensor:\n        b, c2, h, w = x.shape\n        assert c2 % 2 == 0\n        c = c2 // 2\n        return x.view(b, 2, c, h, w).permute(0, 2, 3, 4, 1).contiguous()\n\n    def norm(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        # group norm\n        b, c, h, w = x.shape\n        x = x.view(b, 2, c // 2 * h * w)\n\n        mean = x.mean(dim=2).view(b, 2, 1, 1)\n        std = x.std(dim=2).view(b, 2, 1, 1)\n\n        x = x.view(b, c, h, w)\n\n        return (x - mean) / std, mean, std\n\n    def unnorm(\n        self, x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n    ) -> torch.Tensor:\n        return x * std + mean\n\n    def pad(\n        self, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Tuple[List[int], List[int], int, int]]:\n        _, _, h, w = x.shape\n        w_mult = ((w - 1) | 15) + 1\n        h_mult = ((h - 1) | 15) + 1\n        w_pad = [math.floor((w_mult - w) / 2), math.ceil((w_mult - w) / 2)]\n        h_pad = [math.floor((h_mult - h) / 2), math.ceil((h_mult - h) / 2)]\n        # TODO: fix this type when PyTorch fixes theirs\n        # the documentation lies - this actually takes a list\n        # https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L3457\n        # https://github.com/pytorch/pytorch/pull/16949\n        x = F.pad(x, w_pad + h_pad)\n\n        return x, (h_pad, w_pad, h_mult, w_mult)\n\n    def unpad(\n        self,\n        x: torch.Tensor,\n        h_pad: List[int],\n        w_pad: List[int],\n        h_mult: int,\n        w_mult: int,\n    ) -> torch.Tensor:\n        return x[..., h_pad[0] : h_mult - h_pad[1], w_pad[0] : w_mult - w_pad[1]]\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # print(x.shape)\n        if not x.shape[-1] == 2:\n            raise ValueError(\"Last dimension must be 2 for complex.\")\n\n        # get shapes for unet and normalize\n        x = self.complex_to_chan_dim(x)\n        x, mean, std = self.norm(x)\n        x, pad_sizes = self.pad(x)\n\n        # print(f'before :{x.shape}')\n        x = self.unet(x)\n        # print(f'after :{x.shape}')\n\n\n        # get shapes back and unnormalize\n        x = self.unpad(x, *pad_sizes)\n        x = self.unnorm(x, mean, std)\n        x = self.chan_complex_to_last_dim(x)\n\n        return x\n\n\nclass SensitivityModel(nn.Module):\n    \"\"\"\n    Model for learning sensitivity estimation from k-space data.\n\n    This model applies an IFFT to multichannel k-space data and then a U-Net\n    to the coil images to estimate coil sensitivities. It can be used with the\n    end-to-end variational network.\n    \"\"\"\n\n    def __init__(\n        self,\n        chans: int,\n        num_pools: int,\n        in_chans: int = 2,\n        out_chans: int = 2,\n        drop_prob: float = 0.0,\n        mask_center: bool = True,\n    ):\n        \"\"\"\n        Args:\n            chans: Number of output channels of the first convolution layer.\n            num_pools: Number of down-sampling and up-sampling layers.\n            in_chans: Number of channels in the input to the U-Net model.\n            out_chans: Number of channels in the output to the U-Net model.\n            drop_prob: Dropout probability.\n            mask_center: Whether to mask center of k-space for sensitivity map\n                calculation.\n        \"\"\"\n        super().__init__()\n        self.mask_center = mask_center\n        self.norm_unet = NormUnet(\n            chans,\n            num_pools,\n            in_chans=in_chans,\n            out_chans=out_chans,\n            drop_prob=drop_prob,\n        )\n\n    def chans_to_batch_dim(self, x: torch.Tensor) -> Tuple[torch.Tensor, int]:\n        b, c, h, w, comp = x.shape\n\n        return x.view(b * c, 1, h, w, comp), b\n\n    def batch_chans_to_chan_dim(self, x: torch.Tensor, batch_size: int) -> torch.Tensor:\n        bc, _, h, w, comp = x.shape\n        c = bc // batch_size\n\n        return x.view(batch_size, c, h, w, comp)\n\n    def divide_root_sum_of_squares(self, x: torch.Tensor) -> torch.Tensor:\n        return x / fastmri.rss_complex(x, dim=1).unsqueeze(-1).unsqueeze(1)\n\n    def get_pad_and_num_low_freqs(\n        self, mask: torch.Tensor, num_low_frequencies: Optional[int] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if num_low_frequencies is None or num_low_frequencies == 0:\n            # get low frequency line locations and mask them out\n            squeezed_mask = mask[:, 0, 0, :, 0].to(torch.int8)\n            cent = squeezed_mask.shape[1] // 2\n            # running argmin returns the first non-zero\n            left = torch.argmin(squeezed_mask[:, :cent].flip(1), dim=1)\n            right = torch.argmin(squeezed_mask[:, cent:], dim=1)\n            num_low_frequencies_tensor = torch.max(\n                2 * torch.min(left, right), torch.ones_like(left)\n            )  # force a symmetric center unless 1\n        else:\n            num_low_frequencies_tensor = num_low_frequencies * torch.ones(\n                mask.shape[0], dtype=mask.dtype, device=mask.device\n            )\n\n        pad = (mask.shape[-2] - num_low_frequencies_tensor + 1) // 2\n\n        return pad.type(torch.long), num_low_frequencies_tensor.type(torch.long)\n\n    def forward(\n        self,\n        masked_kspace: torch.Tensor,\n        mask: torch.Tensor,\n        num_low_frequencies: Optional[int] = None,\n    ) -> torch.Tensor:\n        if self.mask_center:\n            pad, num_low_freqs = self.get_pad_and_num_low_freqs(\n                mask, num_low_frequencies\n            )\n            masked_kspace = transforms.batched_mask_center(\n                masked_kspace, pad, pad + num_low_freqs\n            )\n\n        # convert to image space\n        images, batches = self.chans_to_batch_dim(fastmri.ifft2c(masked_kspace))\n\n        # estimate sensitivities\n        return self.divide_root_sum_of_squares(\n            self.batch_chans_to_chan_dim(self.norm_unet(images), batches)\n        )\n\n\nclass VarNet(nn.Module):\n    \"\"\"\n    A full variational network model.\n\n    This model applies a combination of soft data consistency with a U-Net\n    regularizer. To use non-U-Net regularizers, use VarNetBlock.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_cascades: int = 12,\n        sens_chans: int = 8,\n        sens_pools: int = 4,\n        chans: int = 18,\n        pools: int = 4,\n        mask_center: bool = True,\n    ):\n        \"\"\"\n        Args:\n            num_cascades: Number of cascades (i.e., layers) for variational\n                network.\n            sens_chans: Number of channels for sensitivity map U-Net.\n            sens_pools Number of downsampling and upsampling layers for\n                sensitivity map U-Net.\n            chans: Number of channels for cascade U-Net.\n            pools: Number of downsampling and upsampling layers for cascade\n                U-Net.\n            mask_center: Whether to mask center of k-space for sensitivity map\n                calculation.\n        \"\"\"\n        super().__init__()\n\n        self.sens_net = SensitivityModel(\n            chans=sens_chans,\n            num_pools=sens_pools,\n            mask_center=mask_center,\n        )\n        self.cascades = nn.ModuleList(\n            [VarNetBlock(NormUnet(chans, pools)) for _ in range(num_cascades)]\n        )\n\n    def forward(\n        self,\n        masked_kspace: torch.Tensor,\n        mask: torch.Tensor,\n        num_low_frequencies: Optional[int] = None,\n    ) -> torch.Tensor:\n        sens_maps = self.sens_net(masked_kspace, mask, num_low_frequencies)\n        kspace_pred = masked_kspace.clone()\n\n        for cascade in self.cascades:\n            kspace_pred = cascade(kspace_pred, masked_kspace, mask, sens_maps)\n\n        return fastmri.rss(fastmri.complex_abs(fastmri.ifft2c(kspace_pred)), dim=1)\n\n\nclass VarNetBlock(nn.Module):\n    \"\"\"\n    Model block for end-to-end variational network.\n\n    This model applies a combination of soft data consistency with the input\n    model as a regularizer. A series of these blocks can be stacked to form\n    the full variational network.\n    \"\"\"\n\n    def __init__(self, model: nn.Module):\n        \"\"\"\n        Args:\n            model: Module for \"regularization\" component of variational\n                network.\n        \"\"\"\n        super().__init__()\n\n        self.model = model\n        self.dc_weight = nn.Parameter(torch.ones(1))\n\n    def sens_expand(self, x: torch.Tensor, sens_maps: torch.Tensor) -> torch.Tensor:\n        return fastmri.fft2c(fastmri.complex_mul(x, sens_maps))\n\n    def sens_reduce(self, x: torch.Tensor, sens_maps: torch.Tensor) -> torch.Tensor:\n        return fastmri.complex_mul(\n            fastmri.ifft2c(x), fastmri.complex_conj(sens_maps)\n        ).sum(dim=1, keepdim=True)\n\n    def forward(\n        self,\n        current_kspace: torch.Tensor,\n        ref_kspace: torch.Tensor,\n        mask: torch.Tensor,\n        sens_maps: torch.Tensor,\n    ) -> torch.Tensor:\n        zero = torch.zeros(1, 1, 1, 1, 1).to(current_kspace)\n        soft_dc = torch.where(mask, current_kspace - ref_kspace, zero) * self.dc_weight\n        model_term = self.model(\n        model_term = self.sens_expand(\n            self.model(self.sens_reduce(current_kspace, sens_maps)), sens_maps\n        )\n\n        return current_kspace - soft_dc - model_term\n        # return current_kspace - model_term\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.862574Z","iopub.execute_input":"2025-03-17T19:15:22.862871Z","iopub.status.idle":"2025-03-17T19:15:22.891729Z","shell.execute_reply.started":"2025-03-17T19:15:22.862845Z","shell.execute_reply":"2025-03-17T19:15:22.890900Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Metrics </h2>","metadata":{}},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio, structural_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.893136Z","iopub.execute_input":"2025-03-17T19:15:22.893419Z","iopub.status.idle":"2025-03-17T19:15:22.925263Z","shell.execute_reply.started":"2025-03-17T19:15:22.893382Z","shell.execute_reply":"2025-03-17T19:15:22.924639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def psnr(\n    gt: np.ndarray, pred: np.ndarray, maxval: Optional[float] = None\n) -> np.ndarray:\n    \"\"\"Compute Peak Signal to Noise Ratio metric (PSNR)\"\"\"\n    if maxval is None:\n        maxval = gt.max()-gt.min()\n    return peak_signal_noise_ratio(gt, pred, data_range=maxval)\n\n\ndef ssim(\n    gt: np.ndarray, pred: np.ndarray, maxval: Optional[float] = None\n) -> np.ndarray:\n    \"\"\"Compute Structural Similarity Index Metric (SSIM)\"\"\"\n    if not gt.ndim == 3:\n        raise ValueError(\"Unexpected number of dimensions in ground truth.\")\n    if not gt.ndim == pred.ndim:\n        raise ValueError(\"Ground truth dimensions does not match pred.\")\n\n    maxval = gt.max()-gt.min() if maxval is None else maxval\n\n    ssim = 0.0\n    for slice_num in range(gt.shape[0]):\n        ssim = ssim + structural_similarity(gt[slice_num], pred[slice_num], data_range=maxval)\n\n    return ssim / gt.shape[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.926122Z","iopub.execute_input":"2025-03-17T19:15:22.926531Z","iopub.status.idle":"2025-03-17T19:15:22.932876Z","shell.execute_reply.started":"2025-03-17T19:15:22.926504Z","shell.execute_reply":"2025-03-17T19:15:22.932019Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Pytorch Lightning </h2>","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lightning -q\n!pip install wandb -q\nimport pytorch_lightning as pl\nimport wandb\n\nfrom pytorch_lightning.callbacks import Callback, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:22.933696Z","iopub.execute_input":"2025-03-17T19:15:22.934073Z","iopub.status.idle":"2025-03-17T19:15:41.307353Z","shell.execute_reply.started":"2025-03-17T19:15:22.934035Z","shell.execute_reply":"2025-03-17T19:15:41.306340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PrintCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training started!\")\n    def on_train_end(self, trainer, pl_module):\n        print(\"Training is done.\")\n\ncheckpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/model',\n    filename=f\"run 1 {{epoch}}-{{ssim:.2f}}\",\n    monitor='ssim',\n    mode='max',\n    save_top_k=1,\n    save_last=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.308566Z","iopub.execute_input":"2025-03-17T19:15:41.309029Z","iopub.status.idle":"2025-03-17T19:15:41.323051Z","shell.execute_reply.started":"2025-03-17T19:15:41.309001Z","shell.execute_reply":"2025-03-17T19:15:41.322128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VarNet_WCNN_DataModule(pl.LightningDataModule):\n    def __init__(self, train_file, val_file, batch_size, transform, challenge):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_file = train_file\n        self.val_file = val_file\n        self.transform = transform\n        self.challenge = challenge\n\n\n    def setup(self, stage=None):\n        if stage == 'fit':\n            self.train_dataset = Mri_Data(self.train_file, self.transform, self.challenge)\n            self.val_dataset = Mri_Data(self.val_file, self.transform, self.challenge)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.324115Z","iopub.execute_input":"2025-03-17T19:15:41.324381Z","iopub.status.idle":"2025-03-17T19:15:41.330540Z","shell.execute_reply.started":"2025-03-17T19:15:41.324346Z","shell.execute_reply":"2025-03-17T19:15:41.329827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VarNet_WCNN_Model(pl.LightningModule):\n    def __init__(self, Config):\n        super().__init__()\n        self.model = VarNet(num_cascades=4, sens_chans=6, sens_pools=3, chans=15, pools=3)\n        self.loss = fastmri.SSIMLoss()\n        self.config = Config\n        self.automatic_optimization = False\n        self.table = wandb.Table(columns=['Ground_truth', 'Prediction', 'PSNR', 'SSIM', 'Run'])\n        self.acum_step = 8\n        \n\n    def forward(self, masked_kspace, mask):\n        return self.model(masked_kspace, mask)\n\n\n    def training_step(self, batch, batch_idx):\n        masked_kspace, mask, target, *_, max_value, _ = batch\n        if masked_kspace.dim() == 6:\n            masked_kspace = masked_kspace.squeeze(0)\n            mask = mask.squeeze(0)\n            target = target.squeeze(0)\n            max_value = max_value.squeeze(0)\n\n        total_loss = 0\n        optim = self.optimizers()\n        for x, y, z, w in self.generator(masked_kspace, mask, target, max_value, self.acum_step):\n            output = self(x, y)\n            z, output = transforms.center_crop_to_smallest(z, output)\n            l1 = F.l1_loss(output.unsqueeze(1), z.unsqueeze(1))\n            l2 = self.loss(output.unsqueeze(1), z.unsqueeze(1), data_range=w)\n            total_loss += (l1.item() + l2.item())\n            self.manual_backward(l1+l2)\n            optim.step()\n            optim.zero_grad()\n        self.log('train_loss', total_loss, on_step=True, on_epoch=True, batch_size=self.config.batch_size)\n\n\n    def validation_step(self, batch, batch_idx):\n        masked_kspace, mask, target, *_, max_value, _ = batch\n        if masked_kspace.dim() == 6:\n            masked_kspace = masked_kspace.squeeze(0)\n            mask = mask.squeeze(0)\n            target = target.squeeze(0)\n            max_value = max_value.squeeze(0)\n        \n        PSNR, SSIM, val_loss, Step = 0., 0., 0., 0\n        for x, y, z, w in self.generator(masked_kspace, mask, target, max_value, 4):\n            output = self(x, y)\n            z, output = transforms.center_crop_to_smallest(z, output)\n            l1 = F.l1_loss(output.unsqueeze(1), z.unsqueeze(1)).item()\n            l2 = self.loss(output.unsqueeze(1), z.unsqueeze(1), data_range=w).item()\n            val_loss += (l1+l2)\n            z, output = z.detach().cpu().numpy(), output.detach().cpu().numpy()\n            PSNR += psnr(z, output)\n            SSIM += ssim(z, output)\n            Step += 1\n#             if self.trainer.is_last_batch and (self.current_epoch+1) % 5 == 0:\n#                 self.update_table(z, output)\n        \n        metrics = {'val loss': val_loss, 'psnr': PSNR/Step, 'ssim': SSIM/Step}\n        self.log_dict(metrics, on_step=True, on_epoch=True, batch_size=self.config.batch_size)\n\n\n    def on_train_epoch_end(self):\n        lr_scheduler = self.lr_schedulers()\n        lr_scheduler.step()\n        avg_train_loss = self.trainer.callback_metrics['train_loss']\n#         wandb.log({'train_loss':avg_train_loss}, step=self.current_epoch)\n        print(f\"Train Loss: {avg_train_loss}\")\n        \n\n    def on_validation_epoch_end(self):\n        avg_val_loss = self.trainer.callback_metrics['val loss']\n        avg_psnr = self.trainer.callback_metrics['psnr']\n        avg_ssim = self.trainer.callback_metrics['ssim']\n        print(f\"\\nepoch: {self.current_epoch+1}/{self.config.epochs}\")\n        print(f\"Val Loss: {avg_val_loss}\")\n        print(f\"PSNR: {avg_psnr}\")\n        print(f\"SSIM: {avg_ssim}\")\n        optimizer = self.optimizers()\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Current learning rate: {current_lr}\")\n#         wandb.log({'val loss':avg_val_loss, 'PSNR':avg_psnr, 'SSIM':avg_ssim}, step=self.current_epoch)\n    \n    \n    def on_train_end(self):\n#         wandb.log({\"Results\": self.table})\n        pass\n\n\n    def configure_optimizers(self):\n        optim = torch.optim.Adam(self.model.parameters(), lr=self.config.lr, weight_decay=0.0)\n        scheduler = torch.optim.lr_scheduler.StepLR(optim, self.config.lr_step_size, self.config.lr_gamma)\n        return {'optimizer':optim, 'lr_scheduler':scheduler}\n    \n    \n    def update_table(self, target, output):\n        for index in range(target.shape[0]):\n            norm_target = self.normalize_image(target[index])\n            norm_output = self.normalize_image(output[index])\n            snr = peak_signal_noise_ratio(target[index], output[index], data_range=target[index].max()-target[index].min())\n            sim = structural_similarity(target[index], output[index], data_range=target[index].max()-target[index].min())\n            self.table.add_data(wandb.Image(norm_target), wandb.Image(norm_output), snr, sim, self.config.run)\n\n\n    def generator(self, x, y, z, w, acum_step):\n        length = x.shape[0]\n        start = 0\n        while start < length:\n            if start + acum_step <= length:\n                yield x[start:start + acum_step], y[start:start + acum_step], z[start:start + acum_step], w[start:start + acum_step]\n            else:\n                yield x[start:], y[start:], z[start:], w[start:]\n                break\n            start += acum_step\n\n\n    def normalize_image(self, img):\n        img = img.astype(np.float32)\n        img = (img - img.min()) / (img.max() - img.min())\n        return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.331532Z","iopub.execute_input":"2025-03-17T19:15:41.331759Z","iopub.status.idle":"2025-03-17T19:15:41.351064Z","shell.execute_reply.started":"2025-03-17T19:15:41.331736Z","shell.execute_reply":"2025-03-17T19:15:41.350193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Training </h2>","metadata":{}},{"cell_type":"code","source":"class Config:\n    batch_size = 1\n    epochs = 15\n    image_size = 256\n    lr = 0.001\n    lr_gamma = 0.2\n    lr_step_size = 10\n    run = 7\n    challenge = 'multicoil'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.352106Z","iopub.execute_input":"2025-03-17T19:15:41.352355Z","iopub.status.idle":"2025-03-17T19:15:41.364553Z","shell.execute_reply.started":"2025-03-17T19:15:41.352330Z","shell.execute_reply":"2025-03-17T19:15:41.363897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Mask = create_mask_for_mask_type('equispaced_fraction', [0.1171875], [2])\ntrain_transform = DataTransform(uniform_train_resolution=[Config.image_size, Config.image_size], mask_func=Mask, use_seed=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.369612Z","iopub.execute_input":"2025-03-17T19:15:41.369895Z","iopub.status.idle":"2025-03-17T19:15:41.378198Z","shell.execute_reply.started":"2025-03-17T19:15:41.369869Z","shell.execute_reply":"2025-03-17T19:15:41.377539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer=pl.Trainer(\n    accelerator='auto', max_epochs=Config.epochs,\n    deterministic=True, callbacks=[PrintCallback(), checkpoint_callback],\n    # limit_train_batches=1,\n    # limit_val_batches=1,\n    num_sanity_val_steps=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.379058Z","iopub.execute_input":"2025-03-17T19:15:41.379298Z","iopub.status.idle":"2025-03-17T19:15:41.449405Z","shell.execute_reply.started":"2025-03-17T19:15:41.379274Z","shell.execute_reply":"2025-03-17T19:15:41.448801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_MODULE = VarNet_WCNN_DataModule(train_file, val_file, Config.batch_size ,train_transform, Config.challenge)\nMODEL_MODULE = VarNet_WCNN_Model(Config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:15:41.450516Z","iopub.execute_input":"2025-03-17T19:15:41.450860Z","iopub.status.idle":"2025-03-17T19:15:41.515547Z","shell.execute_reply.started":"2025-03-17T19:15:41.450823Z","shell.execute_reply":"2025-03-17T19:15:41.514823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(MODEL_MODULE, DATA_MODULE)\n# wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:15:41.516443Z","iopub.execute_input":"2025-03-17T19:15:41.516700Z","iopub.status.idle":"2025-03-17T19:19:01.716935Z","shell.execute_reply.started":"2025-03-17T19:15:41.516675Z","shell.execute_reply":"2025-03-17T19:19:01.715868Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Inference </h2>","metadata":{}},{"cell_type":"code","source":"# img_size = 200\n# Mask = create_mask_for_mask_type('equispaced_fraction', [0.1171875], [2])\n# val_transform = DataTransform(uniform_train_resolution=[img_size, img_size], mask_func=Mask, use_seed=True)\n# MODEL_MODULE = VarNet_WCNN_Model.load_from_checkpoint(checkpoint_path='/kaggle/input/fastmri-trained-models/brain_FLAIR_ssim0.86.ckpt', Config=Config)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:01.718584Z","iopub.execute_input":"2025-03-17T19:19:01.719036Z","iopub.status.idle":"2025-03-17T19:19:01.724835Z","shell.execute_reply.started":"2025-03-17T19:19:01.718993Z","shell.execute_reply":"2025-03-17T19:19:01.723638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# val_dataset = Mri_Data(val_file, train_transform, 'multicoil')\n# dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2)\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:01.726188Z","iopub.execute_input":"2025-03-17T19:19:01.726579Z","iopub.status.idle":"2025-03-17T19:19:01.739975Z","shell.execute_reply.started":"2025-03-17T19:19:01.726528Z","shell.execute_reply":"2025-03-17T19:19:01.738795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# masked_kspace, mask, target, *_, max_value, _ = next(iter(dataloader))\n# masked_kspace = masked_kspace.squeeze(0)\n# mask = mask.squeeze(0)\n# target = target.squeeze(0)\n# max_value = max_value.squeeze(0)\n# print(masked_kspace.shape, mask.shape)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:01.741061Z","iopub.execute_input":"2025-03-17T19:19:01.741392Z","iopub.status.idle":"2025-03-17T19:19:07.706622Z","shell.execute_reply.started":"2025-03-17T19:19:01.741354Z","shell.execute_reply":"2025-03-17T19:19:07.705583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output = np.zeros((target.shape[0], img_size, img_size))\n# MODEL_MODULE.eval()\n# with torch.no_grad():\n#     for i in range(target.shape[0]):\n#         x, y = masked_kspace[i].unsqueeze(dim=0), mask[i].unsqueeze(dim=0)\n#         pred = MODEL_MODULE(x.to(device), y.to(device))\n#         output[i] = pred.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.708104Z","iopub.execute_input":"2025-03-17T19:19:07.708421Z","iopub.status.idle":"2025-03-17T19:19:07.712685Z","shell.execute_reply.started":"2025-03-17T19:19:07.708384Z","shell.execute_reply":"2025-03-17T19:19:07.711852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# target, output = transforms.center_crop_to_smallest(target, output)\n# target = target.numpy()\n# zero_filling = fastmri.rss(fastmri.complex_abs(fastmri.ifft2c(masked_kspace)), dim=1)\n# print(output.shape, target.shape)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.714286Z","iopub.execute_input":"2025-03-17T19:19:07.714744Z","iopub.status.idle":"2025-03-17T19:19:07.723828Z","shell.execute_reply.started":"2025-03-17T19:19:07.714717Z","shell.execute_reply":"2025-03-17T19:19:07.722993Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# index = 17\n\n# plt.figure(figsize=(10, 10))\n# plt.subplot(1,3,1)\n# plt.imshow(zero_filling[index], cmap='gray')\n# plt.title('Zero-Fill Reconstruction')\n\n# plt.subplot(1,3,2)\n# plt.imshow(target[index], cmap='gray')\n# plt.title('Ground-Truth')\n\n# plt.subplot(1,3,3)\n# plt.imshow(output[index], cmap='gray')\n# plt.title('Prediction')\n\n# print(peak_signal_noise_ratio(target[index], output[index], data_range=target[index].max()))\n# print(structural_similarity(target[index], output[index], data_range=target[index].max()))","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.724894Z","iopub.execute_input":"2025-03-17T19:19:07.725249Z","iopub.status.idle":"2025-03-17T19:19:07.733547Z","shell.execute_reply.started":"2025-03-17T19:19:07.725223Z","shell.execute_reply":"2025-03-17T19:19:07.732824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.imshow(np.abs(target[index]-output[index]), cmap='viridis')\n# plt.colorbar()\n# plt.title('Error Maps')","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.734708Z","iopub.execute_input":"2025-03-17T19:19:07.735374Z","iopub.status.idle":"2025-03-17T19:19:07.744465Z","shell.execute_reply.started":"2025-03-17T19:19:07.735335Z","shell.execute_reply":"2025-03-17T19:19:07.743678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mask_2d = np.squeeze(mask[0])\n\n# plt.figure(figsize=(6, 4))\n# plt.imshow(mask_2d[np.newaxis], aspect='auto', cmap='gray')\n# plt.title('Mask')\n# plt.colorbar()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.745486Z","iopub.execute_input":"2025-03-17T19:19:07.745793Z","iopub.status.idle":"2025-03-17T19:19:07.753752Z","shell.execute_reply.started":"2025-03-17T19:19:07.745753Z","shell.execute_reply":"2025-03-17T19:19:07.753024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mask_2d.shape","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.754621Z","iopub.execute_input":"2025-03-17T19:19:07.754905Z","iopub.status.idle":"2025-03-17T19:19:07.765106Z","shell.execute_reply.started":"2025-03-17T19:19:07.754879Z","shell.execute_reply":"2025-03-17T19:19:07.764306Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output = self.forward(masked_kspace, mask)\n# target, output = transforms.center_crop_to_smallest(target, output)\n# val_loss = self.loss(output.unsqueeze(1), target.unsqueeze(1), data_range=max_value)\n# target, output = target.detach().cpu().numpy(), output.detach().cpu().numpy()\n# PSNR = psnr(target, output)\n# SSIM = ssim(target, output)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.766309Z","iopub.execute_input":"2025-03-17T19:19:07.766661Z","iopub.status.idle":"2025-03-17T19:19:07.777672Z","shell.execute_reply.started":"2025-03-17T19:19:07.766624Z","shell.execute_reply":"2025-03-17T19:19:07.776828Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = h5py.File(train_file[10], 'r')\n# kspace = data['kspace'][()]\n# # target = data['reconstruction_rss'][()]\n# kspace.shape","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.778703Z","iopub.execute_input":"2025-03-17T19:19:07.778992Z","iopub.status.idle":"2025-03-17T19:19:07.787375Z","shell.execute_reply.started":"2025-03-17T19:19:07.778968Z","shell.execute_reply":"2025-03-17T19:19:07.786699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # fig, axes = plt.subplots(3,3, figsize=(12, 8))\n# # idx = 0\n# # axes = axes.flatten()\n# # for i in range(9):\n# #     axes[i].imshow(target[i+idx, 28:228, 28:228], cmap='gray')\n\n# # plt.tight_layout()\n# # plt.show()\n\n# slice_idx = kspace.shape[0] // 2  # Middle slice\n# coil_idx = 0  # First coil\n\n# # Extract the k-space data for the selected slice and coil\n# kspace_slice = kspace[slice_idx, coil_idx, :, :]\n\n# # Compute magnitude and apply log transformation\n# kspace_magnitude = np.abs(kspace_slice)\n# # kspace_log = np.log1p(kspace_magnitude)\n\n# # Crop the central low-frequency region (optional)\n# crop_size = 125  # Adjust based on visualization needs\n# h, w = kspace_log.shape\n# center_h, center_w = h // 2, w // 2\n# kspace_cropped = kspace_log[center_h - crop_size:center_h + crop_size, \n#                             center_w - crop_size:center_w + crop_size]\n\n# # Plot the full k-space magnitude and the zoomed-in low-frequency region\n# fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n\n# ax[0].imshow(kspace_log, cmap='gray')\n# ax[0].set_title(\"Full K-Space (Log Scale)\")\n# ax[0].axis(\"off\")\n\n# ax[1].imshow(kspace_cropped, cmap=\"gray\")\n# ax[1].set_title(\"Low-Frequency Region (Zoomed)\")\n# ax[1].axis(\"off\")\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.788470Z","iopub.execute_input":"2025-03-17T19:19:07.788811Z","iopub.status.idle":"2025-03-17T19:19:07.798464Z","shell.execute_reply.started":"2025-03-17T19:19:07.788761Z","shell.execute_reply":"2025-03-17T19:19:07.797759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# acq = ('CORPDFS_FBK', 'CORPD_FBK')\n\n# fn, acquisition = [], []\n# for file in val_file:\n#     with h5py.File(file, 'r') as data:\n#         attr = dict(data.attrs)\n#         fn.append(file)\n#         acquisition.append(attr['acquisition'])\n        \n# data_frame = pd.DataFrame({'file_name':fn, 'acquisition':acquisition})\n# data_frame.to_csv('val_M4Raw.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.799301Z","iopub.execute_input":"2025-03-17T19:19:07.799558Z","iopub.status.idle":"2025-03-17T19:19:07.808697Z","shell.execute_reply.started":"2025-03-17T19:19:07.799532Z","shell.execute_reply":"2025-03-17T19:19:07.807914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wandb.login(key='7bd60122a5756789b39039e92a54c5c4a3693eee')\n# wandb.init(\n#     project = 'Thesis-fastMRI',\n#     name = f\"run {Config.run}\",\n#     config = {\n#         'epochs':Config.epochs,\n#         'lr':Config.lr,\n#         'batch':Config.batch_size,\n#         'w8_decay':0.0,\n#         'optim':'Adam',\n#         'Model':'VarNet_WCNN',\n#         'Dataset':'Stanford 2D FSE'\n#     }\n# )","metadata":{"execution":{"iopub.status.busy":"2025-03-17T19:19:07.809680Z","iopub.execute_input":"2025-03-17T19:19:07.810029Z","iopub.status.idle":"2025-03-17T19:19:07.820536Z","shell.execute_reply.started":"2025-03-17T19:19:07.809990Z","shell.execute_reply":"2025-03-17T19:19:07.819731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# slice_idx = kspace.shape[0] // 2  # Middle slice\n# kspace_slice = kspace[slice_idx]\n\n# # For k-space visualization (choose first coil or combine)\n# if kspace.shape[1] > 1:\n#     # Using first coil for k-space visualization\n#     kspace_vis = kspace_slice[0]\n# else:\n#     kspace_vis = kspace_slice\n\n# # Compute magnitude and apply log transformation for k-space visualization\n# kspace_magnitude = np.abs(kspace_vis)\n# kspace_log = np.log1p(kspace_magnitude)  # log(1 + |k-space|)\n\n# # Shift zero-frequency component to center for better visualization\n# kspace_centered = np.fft.fftshift(kspace_log)\n\n# # To get the image from k-space (reconstruct the image from all coils)\n# images = []\n# for i in range(kspace_slice.shape[0]):  # Loop through coils\n#     # Apply inverse Fourier transform\n#     img = np.fft.ifft2(kspace_slice[i])\n#     # Take absolute value (magnitude)\n#     img_abs = np.abs(img)\n#     images.append(img_abs)\n\n# # Combine coil images using root sum of squares (if multiple coils)\n# if len(images) > 1:\n#     image_combined = np.sqrt(np.sum(np.square(np.array(images)), axis=0))\n# else:\n#     image_combined = images[0]\n\n# # Create figure with two side-by-side plots\n# fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# # Plot k-space\n# ax[0].imshow(kspace_centered, cmap='gray')\n# ax[0].set_title('K-space (Log Scale, Centered)')\n# ax[0].axis('off')\n\n# # Plot reconstructed image\n# ax[1].imshow(image_combined, cmap='gray')\n# ax[1].set_title('Reconstructed MRI Image')\n# ax[1].axis('off')\n\n# # Add y-axis arrow like in your reference image\n# ax[1].arrow(10, image_combined.shape[0] - 20, 0, -30, head_width=10, \n#            head_length=10, fc='white', ec='white', linewidth=2)\n# ax[1].text(25, image_combined.shape[0] - 35, 'y', color='white', fontsize=14)\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:19:07.821564Z","iopub.execute_input":"2025-03-17T19:19:07.821852Z","iopub.status.idle":"2025-03-17T19:19:07.830291Z","shell.execute_reply.started":"2025-03-17T19:19:07.821826Z","shell.execute_reply":"2025-03-17T19:19:07.829574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}